# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/backtest.ipynb (unless otherwise specified).

__all__ = ['MetricABC', 'NLLScoreline', 'NLLOutcome', 'Backtest']

# Cell
import abc
import itertools

import numpy as np
import mezzala

# Cell


class MetricABC:
    @abc.abstractmethod
    def evaluate_one(self, test, predictions):
        """ Evaluate a single match """
        return 1.0

    def evaluate(self, test, predictions):
        """ Evaluate a set of matches """
        return [self.evaluate_one(t, p) for t, p in zip(test, predictions)]


class NLLScoreline(MetricABC):
    def evaluate_one(self, test, predictions):
        home_goals, away_goals = test['home_goals'], test['away_goals']
        scoreline_pred, *__ = [
            p for p in predictions
            if p.home_goals == home_goals
            and p.away_goals == away_goals
        ]
        return -np.log(scoreline_pred.probability)


class NLLOutcome(MetricABC):
    def evaluate_one(self, test, predictions):
        outcome = mezzala.scoreline_to_outcome(test['home_goals'], test['away_goals'])
        outcome_pred = mezzala.scorelines_to_outcomes(predictions)[outcome]
        return -np.log(outcome_pred.probability)

# Cell


class Backtest:
    def __init__(self, models, metrics):
        self.models = models
        self.metrics = metrics

    def backtest(self, league_ids, dates):
        results = []
        for model, date in itertools.product(self.models, dates):
            train, test = model.fetch_data(league_ids, date)
            model.fit(train)
            predictions = model.predict(test)

            results.append({
                'model': str(model),  # idk...
                'date': date,
                'predictions': predictions,
                **{metric.__class__.__name__: metric.evaluate(test, predictions)
                   for metric in self.metrics}
            })
        return results